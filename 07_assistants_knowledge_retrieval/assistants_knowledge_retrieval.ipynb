{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistants API - Knowledge Retrieval \n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\n",
    "\n",
    "https://community.openai.com/t/new-assistants-api-a-potential-replacement-for-low-level-rag-style-content-generation/475677 \n",
    "\n",
    "Watch:\n",
    "\n",
    "https://youtu.be/5rcjGjgJNQc?t=600&si=d9OtX0nMi2Rv0fQV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client : OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Retrieval\n",
    "\n",
    "Retrieval augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. Once a file is uploaded and passed to the Assistant, OpenAI will automatically chunk your documents, index and store the embeddings, and implement vector search to retrieve relevant content to answer user queries.\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/knowledge-retrieval \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "The model then decides when to retrieve content based on the user Messages. The Assistants API automatically chooses between two retrieval techniques:\n",
    "\n",
    "1. it either passes the file content in the prompt for short documents, or\n",
    "2. performs a vector search for longer documents\n",
    "\n",
    "Retrieval currently optimizes for quality by adding all relevant content to the context of model calls. We plan to introduce other retrieval strategies to enable developers to choose a different tradeoff between retrieval quality and model usage cost.\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/how-it-works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Upload the file and Create an Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-2p7oFAVPX1IPQzOW35mP3SLI', bytes=48802, created_at=1700047815, filename='zia_profile.pdf', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "from openai.types.beta import Assistant\n",
    "\n",
    "# Upload a file with an \"assistants\" purpose\n",
    "file = client.files.create(\n",
    "  file=open(\"zia_profile.pdf\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "print(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant: Assistant = client.beta.assistants.create(\n",
    "  name=\"Student Support Assistant\",\n",
    "  instructions=\"You are a student support chatbot. Use your knowledge base to best respond to student queries about Zia U. Khan.\",\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_UrU3fNWx1yMPGDMUoEuYDpbQ', created_at=1700048142, metadata={}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "from openai.types.beta.thread import Thread\n",
    "\n",
    "thread: Thread  = client.beta.threads.create()\n",
    "\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add a Message to a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.threads.thread_message import ThreadMessage\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"When and which city Zia U. Khan was born?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.threads.run import Run\n",
    "\n",
    "run: Run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Pakistani. The user is the student of PIAIC.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Check the Run status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2yATUyChhncfYC7AQh1MfGMt', assistant_id='asst_WZFG0ig0aVAGWMWfHrw3k5Ze', cancelled_at=None, completed_at=None, created_at=1700048421, expires_at=None, failed_at=1700048433, file_ids=['file-2p7oFAVPX1IPQzOW35mP3SLI'], instructions='Please address the user as Pakistani. The user is the student of PIAIC.', last_error=LastError(code='rate_limit_exceeded', message='Rate limit reached for gpt-3.5-turbo-1106 in organization org-9oP4S86HpNNbJMudY026ckbW on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'), metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1700048421, status='failed', thread_id='thread_UrU3fNWx1yMPGDMUoEuYDpbQ', tools=[ToolAssistantToolsRetrieval(type='retrieval')])\n"
     ]
    }
   ],
   "source": [
    "run: Run = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "\n",
    "print(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Display the Assistant's Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: When and which city Zia U. Khan was born?\n",
      "assistant: I'll need to open the file to find this information. Let me do that now.\n"
     ]
    }
   ],
   "source": [
    "# from openai.resources.beta.threads.messages.messages import SyncCursorPage \n",
    "\n",
    "messages: list[ThreadMessage] = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "\n",
    "for m in reversed(messages.data):\n",
    "  print(m.role + \": \" + m.content[0].text.value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
